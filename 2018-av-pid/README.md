# Audio-Visual Speech Integration: Synergy & Redundancy :eye: :ear:

Combining different sources of information is fundamental to many aspects of behavior, from our ability to picking up a ringing mobile phone to communicating with a friend in a busy environment. Here we have studied the integration of auditory and visual speech information. Our work demonstrates that integration relies upon two different representational interactions. One system conveys redundant information by representing information that is common to both auditory and visual modalities. The other system, which is supported by a different brain area, represents synergistic information by conveying greater information than the linear summation of individual auditory and visual information. Further we show that these pathways offer substantial behavioral advantage. These fresh insights have been achieved by applying to brain imaging data a recently developed and validated approach, Partial Information Decomposition. 
This novel insight opens new ways to enhance our understanding of the mechanisms underlying information integration, a fundamental aspect of brain function. We also believe that this methodology provides a novel and principled way to quantify the interactions between representations of multiple stimulus features in the brain.

**keywords**
#audiovisual #speech #integration #information theory #mutual information (MI) #PID (Partial Information Decomposition) #redundancy #synergy

[Park H, Ince RAA, Schyns PG, Thut G, Gross J (2018) PLoS Biology. Representational interactions during audiovisual speech entrainment: Redundancy in left posterior superior temporal gyrus and synergy in left motor cortex](https://doi.org/10.1371/journal.pbio.2006558)
